{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ SETUP & IMPORTS\n",
        "!pip install fuzzywuzzy python-Levenshtein markovify scikit-learn networkx tqdm pyarrow\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import networkx as nx\n",
        "from fuzzywuzzy import process\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, top_k_accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE8PjfCY5h_R",
        "outputId": "d2a496c3-9fe1-442b-eefe-80ac327fc766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting unidecode (from markovify)\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markovify-0.9.4-py3-none-any.whl (19 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, unidecode, rapidfuzz, markovify, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 fuzzywuzzy-0.18.0 markovify-0.9.4 python-Levenshtein-0.27.1 rapidfuzz-3.14.1 unidecode-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£ MOUNT GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/campus_data\"\n",
        "DB_PATH = os.path.join(DATA_DIR, \"campus.db\")     # sqlite DB file\n",
        "EMBED_PARQUET = os.path.join(DATA_DIR, \"face_embeddings.parquet\")  # embeddings stored separately\n",
        "MODEL_DIR = DATA_DIR\n",
        "# -------------------------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-aWeIZQ5mdQ",
        "outputId": "adeb6904-a095-4752-8d06-13adb77210f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3Ô∏è‚É£ CSV table mapping\n",
        "csv_files = {\n",
        "    \"card_swipes\": \"card_swipes.csv\",\n",
        "    \"cctv_frames\": \"cctv_frames.csv\",\n",
        "    \"face_embeddings\": \"face_embeddings.csv\",      # large - we'll store as parquet\n",
        "    \"free_text_notes\": \"free_text_notes.csv\",\n",
        "    \"lab_bookings\": \"lab_bookings.csv\",\n",
        "    \"library_checkouts\": \"library_checkouts.csv\",\n",
        "    \"profiles\": \"profiles.csv\",\n",
        "    \"wifi_logs\": \"wifi_associations_logs.csv\"\n",
        "}"
      ],
      "metadata": {
        "id": "UCgZLP195vzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4Ô∏è‚É£ Initialize DB (create if not exists) and optionally import CSVs into DB.\n",
        "def initialize_db(import_csvs=True, store_embeddings_as_parquet=True):\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    if import_csvs:\n",
        "        for table, fname in csv_files.items():\n",
        "            path = os.path.join(DATA_DIR, fname)\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"‚ö†Ô∏è Missing file: {path} (skipped)\")\n",
        "                continue\n",
        "            if table == \"face_embeddings\" and store_embeddings_as_parquet:\n",
        "                # load & save as parquet for efficiency, do not put into sqlite\n",
        "                try:\n",
        "                    df_emb = pd.read_csv(path)\n",
        "                    df_emb.to_parquet(EMBED_PARQUET, index=False)\n",
        "                    print(f\"üíæ Saved embeddings to {EMBED_PARQUET} ({len(df_emb)} rows)\")\n",
        "                except Exception as e:\n",
        "                    print(\"‚ùå Failed to convert embeddings to parquet:\", e)\n",
        "            else:\n",
        "                df = pd.read_csv(path)\n",
        "                # write to sqlite\n",
        "                df.to_sql(table, conn, if_exists=\"replace\", index=False)\n",
        "                print(f\"‚úÖ Loaded {table} ({len(df)} rows) into DB\")\n",
        "    conn.close()\n",
        "\n",
        "# Run once (or whenever you want to re-initialize from CSVs)\n",
        "initialize_db(import_csvs=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W_V2DOK54kQ",
        "outputId": "7aab9fd1-b3e1-4ba8-f325-63d1f33758a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded card_swipes (8000 rows) into DB\n",
            "‚úÖ Loaded cctv_frames (7000 rows) into DB\n",
            "üíæ Saved embeddings to /content/drive/MyDrive/campus_data/face_embeddings.parquet (6973 rows)\n",
            "‚úÖ Loaded free_text_notes (7000 rows) into DB\n",
            "‚úÖ Loaded lab_bookings (7000 rows) into DB\n",
            "‚úÖ Loaded library_checkouts (7000 rows) into DB\n",
            "‚úÖ Loaded profiles (7000 rows) into DB\n",
            "‚úÖ Loaded wifi_logs (8000 rows) into DB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£ Helper: load table into DataFrame\n",
        "def load_table_from_db(table_name):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    try:\n",
        "        df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
        "        df[\"source\"] = table_name\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error loading {table_name}: {e}\")\n",
        "        df = pd.DataFrame()\n",
        "    conn.close()\n",
        "    return df"
      ],
      "metadata": {
        "id": "uvaRXTPy58Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6Ô∏è‚É£ Load all available tables (except embeddings which we load separately)\n",
        "dataframes = {}\n",
        "for table in csv_files.keys():\n",
        "    if table == \"face_embeddings\" and os.path.exists(EMBED_PARQUET):\n",
        "        # we will load embeddings lazily if needed\n",
        "        print(f\"‚ÑπÔ∏è Skipping embeddings table here (use load_embeddings() when needed).\")\n",
        "        continue\n",
        "    df = load_table_from_db(table)\n",
        "    if not df.empty:\n",
        "        dataframes[table] = df\n",
        "\n",
        "# Optional loader for embeddings when you need them (does not load by default)\n",
        "def load_embeddings():\n",
        "    if os.path.exists(EMBED_PARQUET):\n",
        "        return pd.read_parquet(EMBED_PARQUET)\n",
        "    # fallback to DB if parquet not present\n",
        "    return load_table_from_db(\"face_embeddings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uvi5s-E6YAB",
        "outputId": "07ed1379-34aa-4ece-9f46-330dbb65bbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è Skipping embeddings table here (use load_embeddings() when needed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7Ô∏è‚É£ Column normalization mapping\n",
        "COLUMN_MAPPING = {\n",
        "    'student_id': 'entity_id',\n",
        "    'user_id': 'entity_id',\n",
        "    'person_id': 'entity_id',\n",
        "    'card_id': 'card_id',\n",
        "    'face_id': 'face_id',\n",
        "    'device_hash': 'device_id',\n",
        "    'location_id': 'location_id',\n",
        "    'loc_id': 'location_id',\n",
        "    'timestamp': 'timestamp',\n",
        "    'time': 'timestamp',\n",
        "    'datetime': 'timestamp',\n",
        "    'date_time': 'timestamp',\n",
        "    'email_id': 'email',\n",
        "    'mail': 'email',\n",
        "    'full_name': 'name',\n",
        "    'user_name': 'name'\n",
        "}\n",
        "\n",
        "def normalize_columns(df):\n",
        "    df = df.rename(columns={c: COLUMN_MAPPING.get(c.lower(), c.lower()) for c in df.columns})\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    return df\n",
        "\n",
        "for k, df in list(dataframes.items()):\n",
        "    df = normalize_columns(df)\n",
        "    df = df.loc[:, ~df.columns.duplicated()].copy()\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    dataframes[k] = df\n"
      ],
      "metadata": {
        "id": "7jNewOTx6fXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8Ô∏è‚É£ Build entity graph for resolution\n",
        "print(\"Building entity graph for resolution...\")\n",
        "G = nx.Graph()\n",
        "for name, df in tqdm(dataframes.items()):\n",
        "    # choose identifier columns that exist in this table\n",
        "    id_cols = ['entity_id', 'card_id', 'face_id', 'device_id', 'email']\n",
        "    id_cols = [c for c in id_cols if c in df.columns]\n",
        "    if not id_cols:\n",
        "        continue\n",
        "    for _, row in df.iterrows():\n",
        "        ids = []\n",
        "        for c in id_cols:\n",
        "            val = row.get(c, None)\n",
        "            if pd.notna(val) and str(val).strip():\n",
        "                ids.append(str(val).strip())\n",
        "        # connect all ids from this record\n",
        "        for i in range(len(ids)):\n",
        "            for j in range(i+1, len(ids)):\n",
        "                G.add_edge(ids[i], ids[j], source=name)\n",
        "\n",
        "print(f\"‚úÖ Graph: nodes={G.number_of_nodes()}, edges={G.number_of_edges()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCxU9KGP6k6f",
        "outputId": "2f44d4f1-72f3-486f-9710-09eef9a9dd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building entity graph for resolution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:02<00:00,  2.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Graph: nodes=30860, edges=62000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9Ô∏è‚É£ Create connected components -> resolved entities\n",
        "entity_groups = list(nx.connected_components(G))\n",
        "entity_map = {}\n",
        "for i, g in enumerate(entity_groups):\n",
        "    eid = f\"E{i+1}\"\n",
        "    for node in g:\n",
        "        entity_map[node] = eid\n",
        "print(f\"‚úÖ Resolved entities: {len(entity_groups)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcTUyr9v6qXP",
        "outputId": "623e54d3-480d-4e2e-d98b-8d16fec61848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Resolved entities: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîü Apply resolved_entity mapping to each dataframe\n",
        "for name, df in dataframes.items():\n",
        "    if 'resolved_entity' not in df.columns:\n",
        "        df['resolved_entity'] = np.nan\n",
        "    for key in ['entity_id', 'card_id', 'face_id', 'device_id', 'email']:\n",
        "        if key in df.columns:\n",
        "            # map values using entity_map; values not found become NaN\n",
        "            mapped = df[key].astype(str).map(entity_map)\n",
        "            df.loc[df['resolved_entity'].isna(), 'resolved_entity'] = mapped\n",
        "    dataframes[name] = df\n",
        "\n",
        "print(\"‚úÖ Applied resolved_entity to dataframes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh6KVS0b6vW_",
        "outputId": "c669471e-fd42-4cdb-d0fe-95869d327602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Applied resolved_entity to dataframes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£1Ô∏è‚É£ Merge all sources into master merged_df\n",
        "merged_df = pd.concat(list(dataframes.values()), ignore_index=True, sort=False)\n",
        "if 'timestamp' in merged_df.columns:\n",
        "    merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'], errors='coerce')\n",
        "merged_df = merged_df.sort_values(['resolved_entity', 'timestamp']).reset_index(drop=True)\n",
        "print(f\"‚úÖ merged_df shape: {merged_df.shape}, unique entities: {merged_df['resolved_entity'].nunique()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AEHPOp77Crf",
        "outputId": "c70c3202-2e8d-46a7-ba59-73cabb3ca79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ merged_df shape: (51000, 36), unique entities: 4860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£2Ô∏è‚É£ Fuzzy normalization for name/email (helps ER)\n",
        "def fuzzy_clean_column(df, column, threshold=90):\n",
        "    if column not in df.columns:\n",
        "        return df\n",
        "    df[column] = df[column].fillna('').astype(str)\n",
        "    unique_vals = [v for v in pd.unique(df[column]) if v and len(v) > 2]\n",
        "    canonical = {}\n",
        "    for val in unique_vals:\n",
        "        if val in canonical: continue\n",
        "        matches = process.extract(val, unique_vals, limit=10)\n",
        "        for m, score in matches:\n",
        "            if score >= threshold:\n",
        "                canonical[m] = val\n",
        "    df[column] = df[column].map(lambda x: canonical.get(x, x))\n",
        "    return df\n",
        "\n",
        "merged_df = fuzzy_clean_column(merged_df, 'name', threshold=92)\n",
        "merged_df = fuzzy_clean_column(merged_df, 'email', threshold=92)\n",
        "print(\"‚úÖ Fuzzy normalization done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asirlMqX7F8f",
        "outputId": "4226ca4c-a065-415c-c52c-5224cda2eb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fuzzy normalization done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£3Ô∏è‚É£ Timeline generation per entity\n",
        "timeline_df = merged_df.groupby('resolved_entity').apply(\n",
        "    lambda g: g.sort_values('timestamp')[['timestamp', 'location_id', 'source']].to_dict('records')\n",
        ").reset_index().rename(columns={0: 'timeline'})\n",
        "print(f\"‚úÖ timelines generated for {len(timeline_df)} entities\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8xTY2yI7Jjf",
        "outputId": "843a8e4a-1b35-4e70-ca24-62c25f972aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ timelines generated for 4860 entities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£4Ô∏è‚É£ Feature engineering for ML\n",
        "df_ml = merged_df.dropna(subset=['resolved_entity', 'timestamp']).copy()\n",
        "df_ml['hour'] = df_ml['timestamp'].dt.hour\n",
        "df_ml['dayofweek'] = df_ml['timestamp'].dt.dayofweek\n",
        "df_ml['month'] = df_ml['timestamp'].dt.month\n",
        "df_ml['location_id'] = df_ml['location_id'].astype(str).fillna(\"unknown\")\n",
        "\n",
        "le_loc = LabelEncoder()\n",
        "df_ml['loc_encoded'] = le_loc.fit_transform(df_ml['location_id'])\n",
        "\n",
        "# Prepare supervised dataset: predict next location (shifted)\n",
        "X = df_ml[['hour', 'dayofweek', 'month', 'loc_encoded']].copy()\n",
        "y = df_ml['loc_encoded'].shift(-1).fillna(df_ml['loc_encoded']).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"‚úÖ Train/test prepared\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYk_EYYx-R-w",
        "outputId": "9dd1aee3-5d85-4a97-ebc8-457f243f77e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Train/test prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£5Ô∏è‚É£ Random Forest with class weights & Top-K evaluation\n",
        "classes = np.unique(y_train)\n",
        "class_weights = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train)))\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=120, random_state=42, class_weight=class_weights, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "probs = rf.predict_proba(X_test)          # (n_samples, n_classes)\n",
        "top1_acc = top_k_accuracy_score(y_test, probs, k=1)\n",
        "top3_acc = top_k_accuracy_score(y_test, probs, k=3)\n",
        "\n",
        "print(f\"\\nüéØ RF Top-1 acc: {top1_acc:.3f}, Top-3 acc: {top3_acc:.3f}\")\n",
        "print(classification_report(y_test, rf.predict(X_test)))\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "print(\"Feature importances:\\n\", importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcOr4bk5-Xce",
        "outputId": "2381f712-e5ce-459f-c6d8-b6647fc49f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ RF Top-1 acc: 0.113, Top-3 acc: 0.408\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.04      0.10      0.06       302\n",
            "           1       0.04      0.12      0.06       312\n",
            "           2       0.06      0.13      0.08       312\n",
            "           3       0.05      0.09      0.06       324\n",
            "           4       0.03      0.07      0.04       304\n",
            "           5       0.04      0.15      0.07       311\n",
            "           6       0.06      0.14      0.08       324\n",
            "           7       0.05      0.10      0.06       297\n",
            "           8       0.65      0.11      0.19      4357\n",
            "\n",
            "    accuracy                           0.11      6843\n",
            "   macro avg       0.11      0.11      0.08      6843\n",
            "weighted avg       0.43      0.11      0.15      6843\n",
            "\n",
            "Feature importances:\n",
            " hour           0.501323\n",
            "dayofweek      0.198725\n",
            "month          0.042760\n",
            "loc_encoded    0.257192\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£6Ô∏è‚É£ Build Markov transition matrix\n",
        "print(\"\\n‚úÖ Building Markov chain transition probabilities...\")\n",
        "# ensure loc_encoded exists in merged_df (use same label encoder mapping)\n",
        "if 'loc_encoded' not in merged_df.columns:\n",
        "    # map merged_df location ids using label encoder if possible\n",
        "    merged_df['location_id'] = merged_df['location_id'].astype(str).fillna(\"unknown\")\n",
        "    merged_df['loc_encoded'] = le_loc.transform(merged_df['location_id'])\n",
        "\n",
        "merged_df = merged_df.dropna(subset=['resolved_entity', 'loc_encoded'])\n",
        "merged_df = merged_df.sort_values(['resolved_entity', 'timestamp']).reset_index(drop=True)\n",
        "merged_df['next_loc'] = merged_df.groupby('resolved_entity')['loc_encoded'].shift(-1)\n",
        "\n",
        "transitions_df = merged_df.dropna(subset=['loc_encoded', 'next_loc']).copy()\n",
        "transitions = transitions_df.groupby(['loc_encoded', 'next_loc']).size().unstack(fill_value=0)\n",
        "# avoid zero-rows (if any) - drop\n",
        "transitions = transitions.loc[(transitions.sum(axis=1) > 0), :]\n",
        "markov_matrix = transitions.div(transitions.sum(axis=1), axis=0)\n",
        "print(\"‚úÖ Markov matrix built. Shape:\", markov_matrix.shape)\n",
        "\n",
        "def predict_markov_topk(current_loc_encoded, top_k=3):\n",
        "    if current_loc_encoded not in markov_matrix.index:\n",
        "        return []\n",
        "    probs = markov_matrix.loc[current_loc_encoded].sort_values(ascending=False)\n",
        "    return probs.head(top_k).index.tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxvwVxEW-a4e",
        "outputId": "6336a4c5-48d2-433f-e559-f447441ead53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Building Markov chain transition probabilities...\n",
            "‚úÖ Markov matrix built. Shape: (9, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£7Ô∏è‚É£ Hybrid predictor (RF probs + Markov)\n",
        "def hybrid_predict_topk(features_row, current_loc_encoded, top_k=3, alpha=0.5):\n",
        "    \"\"\"\n",
        "    features_row: 1D array-like with RF features (hour, dayofweek, month, loc_encoded)\n",
        "    current_loc_encoded: int (current loc index)\n",
        "    alpha: weight for RF vs Markov (0..1) where final_score = alpha*rf + (1-alpha)*markov\n",
        "    Returns: list of (loc_idx, score) sorted descending\n",
        "    \"\"\"\n",
        "    # RF probabilities (dictionary loc_idx -> prob)\n",
        "    prob_rf = {}\n",
        "    rf_proba = rf.predict_proba([features_row])[0]  # shape (n_classes,)\n",
        "    classes_rf = rf.classes_\n",
        "    for c_i, p in zip(classes_rf, rf_proba):\n",
        "        prob_rf[int(c_i)] = float(p)\n",
        "\n",
        "    # Markov probabilities for current location\n",
        "    prob_markov = {}\n",
        "    if current_loc_encoded in markov_matrix.index:\n",
        "        series = markov_matrix.loc[current_loc_encoded]\n",
        "        for idx, p in series.items():\n",
        "            prob_markov[int(idx)] = float(p)\n",
        "\n",
        "    # Combine scores across all possible classes (union)\n",
        "    all_classes = set(list(prob_rf.keys()) + list(prob_markov.keys()))\n",
        "    combined = []\n",
        "    for c in all_classes:\n",
        "        r = prob_rf.get(c, 0.0)\n",
        "        m = prob_markov.get(c, 0.0)\n",
        "        score = alpha * r + (1 - alpha) * m\n",
        "        combined.append((c, score))\n",
        "    combined.sort(key=lambda x: x[1], reverse=True)\n",
        "    return combined[:top_k]\n",
        "\n",
        "# example usage:\n",
        "example_row = X_test.iloc[0].tolist()\n",
        "current_loc = int(X_test.iloc[0]['loc_encoded'])\n",
        "print(\"Hybrid top-3 (loc_idx,score):\", hybrid_predict_topk(example_row, current_loc, top_k=3, alpha=0.6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbcFTa_N-lhe",
        "outputId": "15667135-901f-43d3-f63d-865f1f49cbd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid top-3 (loc_idx,score): [(8, 0.388206462031887), (1, 0.12109149777403404), (0, 0.1176710980589481)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£8Ô∏è‚É£ Save merged table and models back to DB / disk\n",
        "# write merged_df to SQL for future quick loading\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "merged_df.to_sql(\"merged_entity_timeline\", conn, if_exists=\"replace\", index=False)\n",
        "conn.close()\n",
        "\n",
        "joblib.dump(rf, os.path.join(MODEL_DIR, \"rf_model.pkl\"))\n",
        "joblib.dump(markov_matrix, os.path.join(MODEL_DIR, \"markov_model.pkl\"))\n",
        "joblib.dump(le_loc, os.path.join(MODEL_DIR, \"label_encoder_loc.pkl\"))\n",
        "print(\"‚úÖ Saved merged table to DB and dumped models to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyP_a669-1Du",
        "outputId": "cb135bbd-1ff5-4ab2-a8ae-4c32124d8e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved merged table to DB and dumped models to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£9Ô∏è‚É£ Incremental helper functions (no manual SQL required)\n",
        "def append_new_csv_rows_to_db(csv_filename):\n",
        "    \"\"\"\n",
        "    Reads CSV at DATA_DIR/csv_filename and appends only the new rows to DB table\n",
        "    Table name is csv_filename without extension.\n",
        "    \"\"\"\n",
        "    table = os.path.splitext(csv_filename)[0]\n",
        "    csv_path = os.path.join(DATA_DIR, csv_filename)\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(\"‚ö†Ô∏è CSV not found:\", csv_path)\n",
        "        return\n",
        "    df_new = pd.read_csv(csv_path)\n",
        "    df_new = normalize_columns(df_new)\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    try:\n",
        "        df_existing = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
        "    except Exception:\n",
        "        # table not present: just write\n",
        "        df_new.to_sql(table, conn, if_exists=\"replace\", index=False)\n",
        "        print(f\"‚úÖ Created table {table} with {len(df_new)} rows\")\n",
        "        conn.close()\n",
        "        return\n",
        "\n",
        "    # use a conservative merge to find rows present only in CSV (naive: compare all columns)\n",
        "    # to reduce false positives, ensure both have same columns and order\n",
        "    common_cols = [c for c in df_new.columns if c in df_existing.columns]\n",
        "    if not common_cols:\n",
        "        # no columns in common -> append all\n",
        "        df_new.to_sql(table, conn, if_exists=\"append\", index=False)\n",
        "        print(f\"üîÑ Appended all rows to {table} (no common columns to diff)\")\n",
        "        conn.close()\n",
        "        return\n",
        "\n",
        "    df_new_sub = df_new[common_cols].astype(str).fillna('')\n",
        "    df_exist_sub = df_existing[common_cols].astype(str).fillna('')\n",
        "\n",
        "    # mark new rows by concatenated key (fast heuristic)\n",
        "    df_new_sub['_key'] = df_new_sub.apply(lambda row: '|'.join(row.values), axis=1)\n",
        "    df_exist_sub['_key'] = df_exist_sub.apply(lambda row: '|'.join(row.values), axis=1)\n",
        "    new_keys = set(df_new_sub['_key']) - set(df_exist_sub['_key'])\n",
        "    if not new_keys:\n",
        "        print(f\"‚úÖ No new rows to append for {table}\")\n",
        "        conn.close()\n",
        "        return\n",
        "    new_rows = df_new.loc[df_new_sub['_key'].isin(new_keys)]\n",
        "    # append\n",
        "    new_rows.to_sql(table, conn, if_exists=\"append\", index=False)\n",
        "    print(f\"üîÑ Appended {len(new_rows)} new rows to {table}\")\n",
        "    conn.close()\n",
        "\n",
        "def sync_all_csvs():\n",
        "    for fname in csv_files.values():\n",
        "        append_new_csv_rows_to_db(fname)\n",
        "\n",
        "# Example: to append new rows after you add/replace CSVs in Drive:\n",
        "# sync_all_csvs()\n"
      ],
      "metadata": {
        "id": "AYu9ozP0-4ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£0Ô∏è‚É£ Lightweight function to update models after new data arrives\n",
        "def retrain_models_from_db(retrain_rf=True, retrain_markov=True, sample_limit=None):\n",
        "    # reload merged data from DB\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    merged = pd.read_sql_query(\"SELECT * FROM merged_entity_timeline\", conn)\n",
        "    conn.close()\n",
        "\n",
        "    # re-normalize and rebuild minimal pipeline (we reuse existing functions)\n",
        "    merged['timestamp'] = pd.to_datetime(merged['timestamp'], errors='coerce')\n",
        "    merged = merged.sort_values(['resolved_entity', 'timestamp']).reset_index(drop=True)\n",
        "    merged = merged.dropna(subset=['resolved_entity', 'timestamp']).copy()\n",
        "    merged['location_id'] = merged['location_id'].astype(str).fillna(\"unknown\")\n",
        "    le = LabelEncoder()\n",
        "    merged['loc_encoded'] = le.fit_transform(merged['location_id'])\n",
        "\n",
        "    # features\n",
        "    merged['hour'] = merged['timestamp'].dt.hour\n",
        "    merged['dayofweek'] = merged['timestamp'].dt.dayofweek\n",
        "    merged['month'] = merged['timestamp'].dt.month\n",
        "    Xr = merged[['hour', 'dayofweek', 'month', 'loc_encoded']].copy()\n",
        "    yr = merged['loc_encoded'].shift(-1).fillna(merged['loc_encoded']).astype(int)\n",
        "\n",
        "    if sample_limit:\n",
        "        Xr = Xr.sample(sample_limit, random_state=42)\n",
        "        yr = yr.loc[Xr.index]\n",
        "\n",
        "    if retrain_rf:\n",
        "        Xtr, Xte, ytr, yte = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
        "        classes = np.unique(ytr)\n",
        "        c_w = dict(zip(classes, compute_class_weight('balanced', classes=classes, y=ytr)))\n",
        "        model = RandomForestClassifier(n_estimators=120, random_state=42, class_weight=c_w, n_jobs=-1)\n",
        "        model.fit(Xtr, ytr)\n",
        "        joblib.dump(model, os.path.join(MODEL_DIR, \"rf_model.pkl\"))\n",
        "        print(\"‚úÖ Retrained & saved RandomForest\")\n",
        "\n",
        "    if retrain_markov:\n",
        "        merged = merged.sort_values(['resolved_entity', 'timestamp']).reset_index(drop=True)\n",
        "        merged['next_loc'] = merged.groupby('resolved_entity')['loc_encoded'].shift(-1)\n",
        "        tdf = merged.dropna(subset=['loc_encoded', 'next_loc']).copy()\n",
        "        trans = tdf.groupby(['loc_encoded', 'next_loc']).size().unstack(fill_value=0)\n",
        "        trans = trans.loc[(trans.sum(axis=1) > 0), :]\n",
        "        mm = trans.div(trans.sum(axis=1), axis=0)\n",
        "        joblib.dump(mm, os.path.join(MODEL_DIR, \"markov_model.pkl\"))\n",
        "        print(\"‚úÖ Recomputed & saved Markov model\")\n",
        "\n",
        "# Example: retrain_models_from_db()"
      ],
      "metadata": {
        "id": "RIoOTCI2-8yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2Ô∏è‚É£1Ô∏è‚É£ Quick API-like helpers for notebook use\n",
        "def predict_next_locations_for_row(df_row, top_k=3, alpha=0.6):\n",
        "    \"\"\"\n",
        "    df_row: a pandas Series row containing timestamp and location_id or loc_encoded\n",
        "    returns: top_k predicted location labels (decoded)\n",
        "    \"\"\"\n",
        "    # build feature vector\n",
        "    hour = int(df_row['timestamp'].hour) if pd.notna(df_row['timestamp']) else 0\n",
        "    dow = int(df_row['timestamp'].dayofweek) if pd.notna(df_row['timestamp']) else 0\n",
        "    month = int(df_row['timestamp'].month) if pd.notna(df_row['timestamp']) else 0\n",
        "    if 'loc_encoded' in df_row and not pd.isna(df_row['loc_encoded']):\n",
        "        loc_enc = int(df_row['loc_encoded'])\n",
        "    else:\n",
        "        loc_enc = le_loc.transform([str(df_row.get('location_id', 'unknown'))])[0]\n",
        "\n",
        "    feats = [hour, dow, month, loc_enc]\n",
        "    combined = hybrid_predict_topk(feats, loc_enc, top_k=top_k, alpha=alpha)\n",
        "    # decode labels back to location ids\n",
        "    results = [(le_loc.inverse_transform([int(c)])[0], float(score)) for c, score in combined]\n",
        "    return results\n",
        "\n",
        "# Usage example for the most recent event of an entity\n",
        "sample_entity = merged_df['resolved_entity'].dropna().sample(1).iloc[0]\n",
        "entity_events = merged_df[merged_df['resolved_entity'] == sample_entity].sort_values('timestamp')\n",
        "if not entity_events.empty:\n",
        "    last_row = entity_events.iloc[-1]\n",
        "    preds = predict_next_locations_for_row(last_row, top_k=3)\n",
        "    print(f\"Predicted next locations for {sample_entity}: {preds}\")\n",
        "\n",
        "# ========= END OF NOTEBOOK =========\n",
        "print(\"All done ‚Äî notebook ready. Use sync_all_csvs() to append new CSV rows and retrain_models_from_db() to update models.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou3mUlVo_JaO",
        "outputId": "cd055a26-0d52-4fbe-93b1-c8fea6fad925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next locations for E3815: [('nan', 0.39318837969871506), ('LAB_101', 0.1237904145350501), ('CAF_01', 0.10507817621308815)]\n",
            "All done ‚Äî notebook ready. Use sync_all_csvs() to append new CSV rows and retrain_models_from_db() to update models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3lLYqwT_Nuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}